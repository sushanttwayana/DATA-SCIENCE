{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2746241,"sourceType":"datasetVersion","datasetId":1674428},{"sourceId":3548798,"sourceType":"datasetVersion","datasetId":2133850},{"sourceId":7968633,"sourceType":"datasetVersion","datasetId":4688632},{"sourceId":8620416,"sourceType":"datasetVersion","datasetId":4438189}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\nsales = pd.read_csv(\"/kaggle/input/walmart-sales/Walmart_Sales.csv\", index_col = 0)\nsales.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-14T15:57:03.591969Z","iopub.execute_input":"2024-09-14T15:57:03.592349Z","iopub.status.idle":"2024-09-14T15:57:04.036588Z","shell.execute_reply.started":"2024-09-14T15:57:03.592309Z","shell.execute_reply":"2024-09-14T15:57:04.035596Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"             Date  Weekly_Sales  Holiday_Flag  Temperature  Fuel_Price  \\\nStore                                                                    \n1      05-02-2010    1643690.90             0        42.31       2.572   \n1      12-02-2010    1641957.44             1        38.51       2.548   \n1      19-02-2010    1611968.17             0        39.93       2.514   \n1      26-02-2010    1409727.59             0        46.63       2.561   \n1      05-03-2010    1554806.68             0        46.50       2.625   \n\n              CPI  Unemployment  \nStore                            \n1      211.096358         8.106  \n1      211.242170         8.106  \n1      211.289143         8.106  \n1      211.319643         8.106  \n1      211.350143         8.106  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Weekly_Sales</th>\n      <th>Holiday_Flag</th>\n      <th>Temperature</th>\n      <th>Fuel_Price</th>\n      <th>CPI</th>\n      <th>Unemployment</th>\n    </tr>\n    <tr>\n      <th>Store</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>05-02-2010</td>\n      <td>1643690.90</td>\n      <td>0</td>\n      <td>42.31</td>\n      <td>2.572</td>\n      <td>211.096358</td>\n      <td>8.106</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12-02-2010</td>\n      <td>1641957.44</td>\n      <td>1</td>\n      <td>38.51</td>\n      <td>2.548</td>\n      <td>211.242170</td>\n      <td>8.106</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>19-02-2010</td>\n      <td>1611968.17</td>\n      <td>0</td>\n      <td>39.93</td>\n      <td>2.514</td>\n      <td>211.289143</td>\n      <td>8.106</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26-02-2010</td>\n      <td>1409727.59</td>\n      <td>0</td>\n      <td>46.63</td>\n      <td>2.561</td>\n      <td>211.319643</td>\n      <td>8.106</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>05-03-2010</td>\n      <td>1554806.68</td>\n      <td>0</td>\n      <td>46.50</td>\n      <td>2.625</td>\n      <td>211.350143</td>\n      <td>8.106</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Dropping duplicates\n\nRemoving duplicates is an essential skill to get accurate counts because often, you don't want to count the same thing multiple times. In this exercise, you'll create some new DataFrames using unique values from sales.\n\nsales is available and pandas is imported as pd.\n\n\n* Remove rows of sales with duplicate pairs of store and type and save as store_types and print the head.\n* Remove rows of sales with duplicate pairs of store and department and save as store_depts and print the head.\n* Subset the rows that are holiday weeks using the is_holiday column, and drop the duplicate dates, saving as holiday_dates.\n* Select the date column of holiday_dates, and print.","metadata":{}},{"cell_type":"code","source":"# Drop duplicate store/type combinations\nstore_types = sales.drop_duplicates(subset = [\"Fuel_Price\", \"CPI\"])\nprint(store_types.head())\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:57:04.038257Z","iopub.execute_input":"2024-09-14T15:57:04.038589Z","iopub.status.idle":"2024-09-14T15:57:04.052680Z","shell.execute_reply.started":"2024-09-14T15:57:04.038555Z","shell.execute_reply":"2024-09-14T15:57:04.051704Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"             Date  Weekly_Sales  Holiday_Flag  Temperature  Fuel_Price  \\\nStore                                                                    \n1      05-02-2010    1643690.90             0        42.31       2.572   \n1      12-02-2010    1641957.44             1        38.51       2.548   \n1      19-02-2010    1611968.17             0        39.93       2.514   \n1      26-02-2010    1409727.59             0        46.63       2.561   \n1      05-03-2010    1554806.68             0        46.50       2.625   \n\n              CPI  Unemployment  \nStore                            \n1      211.096358         8.106  \n1      211.242170         8.106  \n1      211.289143         8.106  \n1      211.319643         8.106  \n1      211.350143         8.106  \n","output_type":"stream"}]},{"cell_type":"code","source":"# Drop duplicate store/department combinations\nstore_depts = sales.drop_duplicates(subset = [\"Unemployment\"])\nprint(store_depts.head())","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:57:04.054151Z","iopub.execute_input":"2024-09-14T15:57:04.054597Z","iopub.status.idle":"2024-09-14T15:57:04.066382Z","shell.execute_reply.started":"2024-09-14T15:57:04.054549Z","shell.execute_reply":"2024-09-14T15:57:04.065351Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"             Date  Weekly_Sales  Holiday_Flag  Temperature  Fuel_Price  \\\nStore                                                                    \n1      05-02-2010    1643690.90             0        42.31       2.572   \n1      02-04-2010    1594968.28             0        62.27       2.719   \n1      02-07-2010    1492418.14             0        80.91       2.669   \n1      01-10-2010    1453329.50             0        71.89       2.603   \n1      07-01-2011    1444732.28             0        48.27       2.976   \n\n              CPI  Unemployment  \nStore                            \n1      211.096358         8.106  \n1      210.820450         7.808  \n1      211.223533         7.787  \n1      211.671989         7.838  \n1      211.404742         7.742  \n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Subset the rows where is_holiday is True and drop duplicate dates\nholiday_dates = sales[sales[\"Holiday_Flag\"] == True].drop_duplicates(subset=\"Date\")\n\n# Print date col of holiday_dates\nprint(holiday_dates)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:57:04.068755Z","iopub.execute_input":"2024-09-14T15:57:04.069101Z","iopub.status.idle":"2024-09-14T15:57:04.082596Z","shell.execute_reply.started":"2024-09-14T15:57:04.069065Z","shell.execute_reply":"2024-09-14T15:57:04.081419Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"             Date  Weekly_Sales  Holiday_Flag  Temperature  Fuel_Price  \\\nStore                                                                    \n1      12-02-2010    1641957.44             1        38.51       2.548   \n1      10-09-2010    1507460.69             1        78.69       2.565   \n1      26-11-2010    1955624.11             1        64.52       2.735   \n1      31-12-2010    1367320.01             1        48.43       2.943   \n1      11-02-2011    1649614.93             1        36.39       3.022   \n1      09-09-2011    1540471.24             1        76.00       3.546   \n1      25-11-2011    2033320.66             1        60.14       3.236   \n1      30-12-2011    1497462.72             1        44.55       3.129   \n1      10-02-2012    1802477.43             1        48.02       3.409   \n1      07-09-2012    1661767.33             1        83.96       3.730   \n\n              CPI  Unemployment  \nStore                            \n1      211.242170         8.106  \n1      211.495190         7.787  \n1      211.748433         7.838  \n1      211.404932         7.838  \n1      212.936705         7.742  \n1      215.861056         7.962  \n1      218.467621         7.866  \n1      219.535990         7.866  \n1      220.265178         7.348  \n1      222.439015         6.908  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"Dazzling duplicate dropping! The holiday weeks correspond to the Superbowl in February, Labor Day in September, Thanksgiving in November, and Christmas in December. Now that the duplicates are removed, it's time to do some counting.","metadata":{}},{"cell_type":"markdown","source":"**Counting categorical variables**\n\nCounting is a great way to get an overview of your data and to spot curiosities that you might not notice otherwise. In this exercise, you'll count the number of each type of store and the number of each department number using the DataFrames you created in the previous exercise:\n\n# Drop duplicate store/type combinations\n\nstore_types = sales.drop_duplicates(subset=[\"store\", \"type\"])\n\n# Drop duplicate store/department combinations\n\nstore_depts = sales.drop_duplicates(subset=[\"store\", \"department\"])\n\nThe store_types and store_depts DataFrames you created in the last exercise are available, and pandas is imported as pd.\n\n\n* Count the number of stores of each store type in store_types.\n* Count the proportion of stores of each store type in store_types.\n* Count the number of stores of each department in store_depts, sorting the counts in descending order.\n* Count the proportion of stores of each department in store_depts, sorting the proportions in descending order.","metadata":{}},{"cell_type":"code","source":"store_types = pd.read_csv(\"/kaggle/input/stores-area-and-sales-data/Stores.csv\", index_col =0\n                         )\nstore_types.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:57:04.083948Z","iopub.execute_input":"2024-09-14T15:57:04.084271Z","iopub.status.idle":"2024-09-14T15:57:04.111868Z","shell.execute_reply.started":"2024-09-14T15:57:04.084231Z","shell.execute_reply":"2024-09-14T15:57:04.110630Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"           Store_Area  Items_Available  Daily_Customer_Count  Store_Sales\nStore ID                                                                 \n1                1659             1961                   530        66490\n2                1461             1752                   210        39820\n3                1340             1609                   720        54010\n4                1451             1748                   620        53730\n5                1770             2111                   450        46620","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Store_Area</th>\n      <th>Items_Available</th>\n      <th>Daily_Customer_Count</th>\n      <th>Store_Sales</th>\n    </tr>\n    <tr>\n      <th>Store ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1659</td>\n      <td>1961</td>\n      <td>530</td>\n      <td>66490</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1461</td>\n      <td>1752</td>\n      <td>210</td>\n      <td>39820</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1340</td>\n      <td>1609</td>\n      <td>720</td>\n      <td>54010</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1451</td>\n      <td>1748</td>\n      <td>620</td>\n      <td>53730</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1770</td>\n      <td>2111</td>\n      <td>450</td>\n      <td>46620</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Count the number of stores of each type\nstore_counts = store_types[\"Store_Area\"].value_counts()\nprint(store_counts)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:57:04.113605Z","iopub.execute_input":"2024-09-14T15:57:04.114045Z","iopub.status.idle":"2024-09-14T15:57:04.124489Z","shell.execute_reply.started":"2024-09-14T15:57:04.113998Z","shell.execute_reply":"2024-09-14T15:57:04.123124Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Store_Area\n1439    5\n1513    5\n1458    5\n1440    5\n1539    5\n       ..\n1509    1\n1339    1\n1590    1\n2169    1\n1387    1\nName: count, Length: 583, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get the proportion of stores of each type\nstore_props = store_types[\"Store_Area\"].value_counts(normalize = True)\nprint(store_props)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:57:04.126038Z","iopub.execute_input":"2024-09-14T15:57:04.127060Z","iopub.status.idle":"2024-09-14T15:57:04.134349Z","shell.execute_reply.started":"2024-09-14T15:57:04.127015Z","shell.execute_reply":"2024-09-14T15:57:04.133105Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Store_Area\n1439    0.005580\n1513    0.005580\n1458    0.005580\n1440    0.005580\n1539    0.005580\n          ...   \n1509    0.001116\n1339    0.001116\n1590    0.001116\n2169    0.001116\n1387    0.001116\nName: proportion, Length: 583, dtype: float64\n","output_type":"stream"}]},{"cell_type":"code","source":"# Count the number of stores for each department and sort\ndept_counts_sorted = store_types[\"Store_Sales\"].value_counts(sort=True)\nprint(dept_counts_sorted)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:57:04.135708Z","iopub.execute_input":"2024-09-14T15:57:04.136093Z","iopub.status.idle":"2024-09-14T15:57:04.147589Z","shell.execute_reply.started":"2024-09-14T15:57:04.136052Z","shell.execute_reply":"2024-09-14T15:57:04.145629Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Store_Sales\n54590    3\n63540    3\n74080    2\n63660    2\n77120    2\n        ..\n50740    1\n77070    1\n74730    1\n68900    1\n54340    1\nName: count, Length: 816, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get the proportion of stores in each department and sort\ndept_props_sorted = store_types[\"Store_Sales\"].value_counts(sort=True, normalize=True)\nprint(dept_props_sorted)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:57:04.149165Z","iopub.execute_input":"2024-09-14T15:57:04.149578Z","iopub.status.idle":"2024-09-14T15:57:04.159680Z","shell.execute_reply.started":"2024-09-14T15:57:04.149530Z","shell.execute_reply":"2024-09-14T15:57:04.158429Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Store_Sales\n54590    0.003348\n63540    0.003348\n74080    0.002232\n63660    0.002232\n77120    0.002232\n           ...   \n50740    0.001116\n77070    0.001116\n74730    0.001116\n68900    0.001116\n54340    0.001116\nName: proportion, Length: 816, dtype: float64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# What percent of sales occurred at each store type?\n\nWhile .groupby() is useful, you can calculate grouped summary statistics without it.\n\nWalmart distinguishes three types of stores: \"supercenters,\" \"discount stores,\" and \"neighborhood markets,\" encoded in this dataset as type \"A,\" \"B,\" and \"C.\" In this exercise, you'll calculate the total sales made at each store type, without using .groupby(). You can then use these numbers to see what proportion of Walmart's total sales were made at each type.\n\nsales is available and pandas is imported as pd.\n\n\n* Calculate the total weekly_sales over the whole dataset.\n* Subset for type \"A\" stores, and calculate their total weekly sales.\n* Do the same for type \"B\" and type \"C\" stores.\n* Combine the A/B/C results into a list, and divide by sales_all to get the proportion of sales by type.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSV files\nstores_df = pd.read_csv('/kaggle/input/walmart-super-market-dataset/Master Data/stores.csv')\ntrain_df = pd.read_csv('/kaggle/input/walmart-super-market-dataset/Master Data/train.csv')\n\n# Merge the two datasets (assuming common column is 'Store')\nmerged_df = pd.merge(train_df, stores_df, on='Store')\n\n# Save the merged dataframe to a new CSV file\nmerged_df.to_csv('store.csv', index=False)\n\nprint(\"CSV files merged and saved as 'store.csv'.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:57:04.162785Z","iopub.execute_input":"2024-09-14T15:57:04.163212Z","iopub.status.idle":"2024-09-14T15:57:05.852315Z","shell.execute_reply.started":"2024-09-14T15:57:04.163165Z","shell.execute_reply":"2024-09-14T15:57:05.850958Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"CSV files merged and saved as 'store.csv'.\n","output_type":"stream"}]},{"cell_type":"code","source":"store_sales = pd.read_csv(\"/kaggle/working/store.csv\", index_col = 0)\nstore_sales","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:57:05.853883Z","iopub.execute_input":"2024-09-14T15:57:05.854318Z","iopub.status.idle":"2024-09-14T15:57:06.173275Z","shell.execute_reply.started":"2024-09-14T15:57:05.854264Z","shell.execute_reply":"2024-09-14T15:57:06.172133Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"       Dept        Date  Weekly_Sales  IsHoliday Type    Size\nStore                                                        \n1         1  2010-02-05      24924.50      False    A  151315\n1         1  2010-02-12      46039.49       True    A  151315\n1         1  2010-02-19      41595.55      False    A  151315\n1         1  2010-02-26      19403.54      False    A  151315\n1         1  2010-03-05      21827.90      False    A  151315\n...     ...         ...           ...        ...  ...     ...\n45       98  2012-09-28        508.37      False    B  118221\n45       98  2012-10-05        628.10      False    B  118221\n45       98  2012-10-12       1061.02      False    B  118221\n45       98  2012-10-19        760.01      False    B  118221\n45       98  2012-10-26       1076.80      False    B  118221\n\n[421570 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dept</th>\n      <th>Date</th>\n      <th>Weekly_Sales</th>\n      <th>IsHoliday</th>\n      <th>Type</th>\n      <th>Size</th>\n    </tr>\n    <tr>\n      <th>Store</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2010-02-05</td>\n      <td>24924.50</td>\n      <td>False</td>\n      <td>A</td>\n      <td>151315</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2010-02-12</td>\n      <td>46039.49</td>\n      <td>True</td>\n      <td>A</td>\n      <td>151315</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2010-02-19</td>\n      <td>41595.55</td>\n      <td>False</td>\n      <td>A</td>\n      <td>151315</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2010-02-26</td>\n      <td>19403.54</td>\n      <td>False</td>\n      <td>A</td>\n      <td>151315</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2010-03-05</td>\n      <td>21827.90</td>\n      <td>False</td>\n      <td>A</td>\n      <td>151315</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>98</td>\n      <td>2012-09-28</td>\n      <td>508.37</td>\n      <td>False</td>\n      <td>B</td>\n      <td>118221</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>98</td>\n      <td>2012-10-05</td>\n      <td>628.10</td>\n      <td>False</td>\n      <td>B</td>\n      <td>118221</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>98</td>\n      <td>2012-10-12</td>\n      <td>1061.02</td>\n      <td>False</td>\n      <td>B</td>\n      <td>118221</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>98</td>\n      <td>2012-10-19</td>\n      <td>760.01</td>\n      <td>False</td>\n      <td>B</td>\n      <td>118221</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>98</td>\n      <td>2012-10-26</td>\n      <td>1076.80</td>\n      <td>False</td>\n      <td>B</td>\n      <td>118221</td>\n    </tr>\n  </tbody>\n</table>\n<p>421570 rows Ã— 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Calc total weekly sales\nsales_all = store_sales[\"Weekly_Sales\"].sum()\n\n# Subset for type A stores, calc total weekly sales\nsales_A = store_sales[store_sales[\"Type\"] == \"A\"][\"Weekly_Sales\"].sum()\n\n# Subset for type B stores, calc total weekly sales\nsales_B = store_sales[store_sales[\"Type\"] == \"B\"][\"Weekly_Sales\"].sum()\n\n# Subset for type C stores, calc total weekly sales\n# sales_C =store_sales[sales[\"Type\"] == \"C\"][\"Weekly_Sales\"].sum()\n\n# Get proportion for each type\nsales_propn_by_type = [sales_A, sales_B] / sales_all\nprint(sales_propn_by_type)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:57:06.174481Z","iopub.execute_input":"2024-09-14T15:57:06.174820Z","iopub.status.idle":"2024-09-14T15:57:06.280490Z","shell.execute_reply.started":"2024-09-14T15:57:06.174784Z","shell.execute_reply":"2024-09-14T15:57:06.279130Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[0.64284903 0.2969624 ]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Calculations with .groupby()**\n\nThe .groupby() method makes life much easier. In this exercise, you'll perform the same calculations as last time, except you'll use the .groupby() method. You'll also perform calculations on data grouped by two variables to see if sales differ by store type depending on if it's a holiday week or not.\n\nsales is available and pandas is loaded as pd.\n\n\n* Group sales by \"type\" and \"is_holiday\", take the sum of weekly_sales, and store as sales_by_type_is_holiday.","metadata":{}},{"cell_type":"code","source":"# From previous step\nsales_sales = store_sales.groupby(\"Type\")[\"Weekly_Sales\"].sum()\n\n# Group by type and is_holiday; calc total weekly sales\nsales_by_type_is_holiday = store_sales.groupby([\"Type\", \"IsHoliday\"])[\"Weekly_Sales\"].sum()\n\nprint(sales_by_type_is_holiday)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:57:06.282058Z","iopub.execute_input":"2024-09-14T15:57:06.282654Z","iopub.status.idle":"2024-09-14T15:57:06.365785Z","shell.execute_reply.started":"2024-09-14T15:57:06.282616Z","shell.execute_reply":"2024-09-14T15:57:06.364574Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Type  IsHoliday\nA     False        4.007612e+09\n      True         3.234028e+08\nB     False        1.847060e+09\n      True         1.536410e+08\nC     False        3.772478e+08\n      True         2.825570e+07\nName: Weekly_Sales, dtype: float64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Multiple grouped summaries**\n\nEarlier in this chapter, you saw that the .agg() method is useful to compute multiple statistics on multiple variables. It also works with grouped data. NumPy, which is imported as np, has many different summary statistics functions, including: np.min, np.max, np.mean, and np.median.\n\nsales is available and pandas is imported as pd.\n\n\n* Import numpy with the alias np.\n* Get the min, max, mean, and median of weekly_sales for each store type using .groupby() and .agg(). Store this as sales_stats. Make sure to use numpy functions!\n* Get the min, max, mean, and median of unemployment and fuel_price_usd_per_l for each store type. Store this as unemp_fuel_stats.","metadata":{}},{"cell_type":"code","source":"# Import numpy with the alias np\nimport numpy as np\n\n# For each store type, aggregate weekly_sales: get min, max, mean, and median\nsales_stats = store_sales.groupby(\"Type\")[\"Weekly_Sales\"].agg([np.min, np.max, np.mean, np.median])\n\n# Print sales_stats\nprint(sales_stats)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:57:06.367003Z","iopub.execute_input":"2024-09-14T15:57:06.367328Z","iopub.status.idle":"2024-09-14T15:57:06.430305Z","shell.execute_reply.started":"2024-09-14T15:57:06.367293Z","shell.execute_reply":"2024-09-14T15:57:06.429340Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"          min        max          mean    median\nType                                            \nA    -4988.94  474330.10  20099.568043  10105.17\nB    -3924.00  693099.36  12237.075977   6187.87\nC     -379.00  112152.35   9519.532538   1149.67\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1941672815.py:5: FutureWarning: The provided callable <function min at 0x7e8f480cd630> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n  sales_stats = store_sales.groupby(\"Type\")[\"Weekly_Sales\"].agg([np.min, np.max, np.mean, np.median])\n/tmp/ipykernel_36/1941672815.py:5: FutureWarning: The provided callable <function max at 0x7e8f480cd510> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n  sales_stats = store_sales.groupby(\"Type\")[\"Weekly_Sales\"].agg([np.min, np.max, np.mean, np.median])\n/tmp/ipykernel_36/1941672815.py:5: FutureWarning: The provided callable <function mean at 0x7e8f480cde10> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n  sales_stats = store_sales.groupby(\"Type\")[\"Weekly_Sales\"].agg([np.min, np.max, np.mean, np.median])\n/tmp/ipykernel_36/1941672815.py:5: FutureWarning: The provided callable <function median at 0x7e8f336595a0> is currently using SeriesGroupBy.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n  sales_stats = store_sales.groupby(\"Type\")[\"Weekly_Sales\"].agg([np.min, np.max, np.mean, np.median])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Print sales_stats\nprint(sales_stats)\n\n# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\nunemp_fuel_stats = store_sales.groupby(\"Type\")[[\"Size\", \"Weekly_Sales\"]].agg([np.min, np.max, np.mean, np.median])\n\n# Print unemp_fuel_stats\nprint(unemp_fuel_stats)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:57:06.431493Z","iopub.execute_input":"2024-09-14T15:57:06.431841Z","iopub.status.idle":"2024-09-14T15:57:06.518421Z","shell.execute_reply.started":"2024-09-14T15:57:06.431806Z","shell.execute_reply":"2024-09-14T15:57:06.517134Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"          min        max          mean    median\nType                                            \nA    -4988.94  474330.10  20099.568043  10105.17\nB    -3924.00  693099.36  12237.075977   6187.87\nC     -379.00  112152.35   9519.532538   1149.67\n       Size                                  Weekly_Sales             \\\n        min     max           mean    median          min        max   \nType                                                                   \nA     39690  219622  182231.285486  202505.0     -4988.94  474330.10   \nB     34875  140167  101818.735827  114533.0     -3924.00  693099.36   \nC     39690   42988   40535.725286   39910.0      -379.00  112152.35   \n\n                              \n              mean    median  \nType                          \nA     20099.568043  10105.17  \nB     12237.075977   6187.87  \nC      9519.532538   1149.67  \n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1711119260.py:5: FutureWarning: The provided callable <function min at 0x7e8f480cd630> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n  unemp_fuel_stats = store_sales.groupby(\"Type\")[[\"Size\", \"Weekly_Sales\"]].agg([np.min, np.max, np.mean, np.median])\n/tmp/ipykernel_36/1711119260.py:5: FutureWarning: The provided callable <function max at 0x7e8f480cd510> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n  unemp_fuel_stats = store_sales.groupby(\"Type\")[[\"Size\", \"Weekly_Sales\"]].agg([np.min, np.max, np.mean, np.median])\n/tmp/ipykernel_36/1711119260.py:5: FutureWarning: The provided callable <function mean at 0x7e8f480cde10> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n  unemp_fuel_stats = store_sales.groupby(\"Type\")[[\"Size\", \"Weekly_Sales\"]].agg([np.min, np.max, np.mean, np.median])\n/tmp/ipykernel_36/1711119260.py:5: FutureWarning: The provided callable <function median at 0x7e8f336595a0> is currently using SeriesGroupBy.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n  unemp_fuel_stats = store_sales.groupby(\"Type\")[[\"Size\", \"Weekly_Sales\"]].agg([np.min, np.max, np.mean, np.median])\n/tmp/ipykernel_36/1711119260.py:5: FutureWarning: The provided callable <function min at 0x7e8f480cd630> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n  unemp_fuel_stats = store_sales.groupby(\"Type\")[[\"Size\", \"Weekly_Sales\"]].agg([np.min, np.max, np.mean, np.median])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Pivoting on one variable**\n\nPivot tables are the standard way of aggregating data in spreadsheets.\n\nIn pandas, pivot tables are essentially another way of performing grouped calculations. That is, the .pivot_table() method is an alternative to .groupby().\n\nIn this exercise, you'll perform calculations using .pivot_table() to replicate the calculations you performed in the last lesson using .groupby().\n\nsales is available and pandas is imported as pd.\n\n\n* Get the mean weekly_sales by type using .pivot_table() and store as mean_sales_by_type.\n* Get the mean and median (using NumPy functions) of weekly_sales by type using .pivot_table() and store as mean_med_sales_by_type.\n* Get the mean of weekly_sales by type and is_holiday using .pivot_table() and store as mean_sales_by_type_holiday.","metadata":{}},{"cell_type":"code","source":"# Pivot for mean weekly_sales by store type and holiday \nmean_sales_by_type_holiday = store_sales.pivot_table(values = \"Weekly_Sales\"  , index = \"Type\", columns = \"IsHoliday\" )\n\n# Print mean_sales_by_type_holiday\nprint(mean_sales_by_type_holiday)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T15:57:06.520220Z","iopub.execute_input":"2024-09-14T15:57:06.520658Z","iopub.status.idle":"2024-09-14T15:57:06.586610Z","shell.execute_reply.started":"2024-09-14T15:57:06.520609Z","shell.execute_reply":"2024-09-14T15:57:06.585410Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"IsHoliday         False         True \nType                                 \nA          20008.746759  21297.517824\nB          12153.067752  13346.164062\nC           9518.528116   9532.963131\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Fill in missing values and sum values with pivot tables**\n\nThe .pivot_table() method has several useful arguments, including fill_value and margins.\n\nfill_value replaces missing values with a real value (known as imputation). What to replace missing values with is a topic big enough to have its own course (Dealing with Missing Data in Python), but the simplest thing to do is to substitute a dummy value.\nmargins is a shortcut for when you pivoted by two variables, but also wanted to pivot by each of those variables separately: it gives the row and column totals of the pivot table contents.\nIn this exercise, you'll practice using these arguments to up your pivot table skills, which will help you crunch numbers more efficiently!\n\n\n* Print the mean weekly_sales by department and type, filling in any missing values with 0.\n* Print the mean weekly_sales by department and type, filling in any missing values with 0 and summing all rows and columns.","metadata":{}},{"cell_type":"code","source":"# Print mean weekly_sales by department and type; fill missing values with 0\nprint(store_sales.pivot_table(values = \"Weekly_Sales\", index = \"Dept\",columns = \"Type\", fill_value=0))","metadata":{"execution":{"iopub.status.busy":"2024-09-14T16:17:07.247358Z","iopub.execute_input":"2024-09-14T16:17:07.248347Z","iopub.status.idle":"2024-09-14T16:17:07.315587Z","shell.execute_reply.started":"2024-09-14T16:17:07.248300Z","shell.execute_reply":"2024-09-14T16:17:07.314263Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Type             A             B             C\nDept                                          \n1     22956.887886  17990.876158   8951.733462\n2     51994.674873  43051.996919  14424.851713\n3     13881.033137  12965.414311    820.276818\n4     32973.814075  21259.895804  13669.370396\n5     26803.448045  21184.602916    767.600774\n...            ...           ...           ...\n95    97094.026043  41304.769202  50641.564872\n96    19900.943552   4745.183226  15766.025431\n97    22093.807101   3727.100717  13419.542809\n98    10979.816195    319.205901   5479.758054\n99      431.443064     25.716667      8.330952\n\n[81 rows x 3 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\nprint(store_sales.pivot_table(values=\"Weekly_Sales\", index=\"Dept\", columns=\"Type\",fill_value = 0, margins = True))","metadata":{"execution":{"iopub.status.busy":"2024-09-14T16:18:08.472780Z","iopub.execute_input":"2024-09-14T16:18:08.473173Z","iopub.status.idle":"2024-09-14T16:18:08.632221Z","shell.execute_reply.started":"2024-09-14T16:18:08.473130Z","shell.execute_reply":"2024-09-14T16:18:08.631012Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Type             A             B             C           All\nDept                                                        \n1     22956.887886  17990.876158   8951.733462  19213.485088\n2     51994.674873  43051.996919  14424.851713  43607.020113\n3     13881.033137  12965.414311    820.276818  11793.698516\n4     32973.814075  21259.895804  13669.370396  25974.630238\n5     26803.448045  21184.602916    767.600774  21365.583515\n...            ...           ...           ...           ...\n96    19900.943552   4745.183226  15766.025431  15210.942761\n97    22093.807101   3727.100717  13419.542809  14255.576919\n98    10979.816195    319.205901   5479.758054   6824.694889\n99      431.443064     25.716667      8.330952    415.487065\nAll   20099.568043  12237.075977   9519.532538  15981.258123\n\n[82 rows x 4 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Magnificent margin making! You are now armed with pivot table skills that can help you compute summaries at multiple grouped levels in one line of code. Note the subtlety in the value of margins here. The column 'All' returns an overall mean for each department, not (A+B)/2. (A+B)/2 would be a mean of means, rather than an overall mean per department!","metadata":{}}]}